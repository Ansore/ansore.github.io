---
title: Softmax回归
tags:
  - 机器学习
  - mxnet
  - softmax
categories:
  - 机器学习
cover: https://img.ansore.top/2022/04/28/6269696188c94.jpg
abbrlink: 9558f536
date: 2019-01-01 14:25:19
---

和线性回归不同，Softmax回归的输出单元从一个变成了多个，且引入了softmax运算使得输出更适合离散值的预测和训练。

### 分类问题

假设一个简单的图像分类问题，输入的图像为灰度、高和宽分别问两个像素。这样每个像素都可以用一个标量来表示。将图像的四个像素分别记为$x_1,x_2,x_3,x_4$，假设这些训练集中图像的真实标签为狗、猫和猪（假设四个像素可以表示出这三种动物），这些标签分别对应离散值$y_1,y_2,y_3$。

通常用离散的数值来表示类别，例如$y_1=1,y_2=2,y_3=3$。这样，一张图像的标签就为1,2和3这三个数值中的一个，虽然可以使回归模型来建模，并叫预测值就近定点化到1,2和3这三个数值中，但是这种连续值到离散值的转化会影响到分类的质量。因此，一般使用更加适合的离散值输出的模型来解决分类问题。

### Softmax回归模型

softmax回归跟线性回归一样将输入特征与权重做线性叠加。与回归模型的一个主要区别是，softmax回归的输出值个数等于标签里的类别数。因为一共有4个特征和3种输出动物类别，所以权重包含12个标量（带下标的$w$）、偏置包含三个标量（带下标的$b$），且对每一个输入计算$o_1,o_2,o_3$这三个输出：

$$o_1 = x_1w_{11}+x_2w_{21}+x_3w_{31}+x_4w_{41}+b_1$$

$$o_2 = x_1w_{12}+x_2w_{22}+x_3w_{32}+x_4w_{42}+b_2$$

$$o_3 = x_1w_{13}+x_2w_{23}+x_3w_{33}+x_4w_{43}+b_3$$

与线性回归一样，softmax回归也是一个单层网络。每个输出都依赖于所有的输入，softmax回归的输出层也是一个全连接层。

### softmax运算

既然分类问题需要得到离散的预测输出，一个简单的办法是将输出值$o_i$当做预测类别$i$的置信度，并将值最大的输出作为对应的类别预测输出，即输出$argmax_io_i$。

然而，直接使用输出层的输出有两点问题。一方面，由于输出层的输出值的范围不确定，难以直观上判断这些值的意义。另一方面，由于真是标签是离散值，这些离散值与不确定范围的输出值之间的误差难以衡量。

softmax运算符（softmax operator）解决了这两个问题。它通过一下方式将输出值变成值为正且和为1的概率分布：

$$\hat{y1},\hat{y2},\hat{y2} = softmax(o_1,o_2,o_3)$$

其中：

$$\hat{y1} = \frac{\exp(o_1)}{\sum_{i=1}^3\exp(o_i)}$$

$$\hat{y2} = \frac{\exp(o_2)}{\sum_{i=1}^3\exp(o_i)}$$

$$\hat{y3} = \frac{\exp(o_3)}{\sum_{i=1}^3\exp(o_i)}$$

容易看出$\hat{y_1}+\hat{y_2}+\hat{y_3}=1$且$0<=\hat{y_1},\hat{y_2},\hat{y_3}<=1$，因此$\hat{y_1},\hat{y_2},\hat{y_3}$是一个合法的概率分布。此外

$$argmax_io_i=argmax_i\hat{y}_i$$

因此softmax运算不改变预测类别输出。

### 单样本分类的矢量计算表达式

假设softmax回归的权重和偏置参数分别为

$$W = \left[
\begin{matrix}
w_{11} & w_{12} & w_{13} \\ 
w_{21} & w_{22} & w_{23} \\ 
w_{31} & w_{32} & w_{33} \\ 
w_{41} & w_{42} & w_{43}
\end{matrix}\right] $$

$$b = \left[\begin{matrix} b_1 & b_2 & b_3 \end{matrix}\right]$$

设宽和高分别为2个像素的图像样本$i$的特征为

$$x^{(i)} = \left[\begin{matrix} x_1^{(i)} & x_2^{(i)} & x_3^{(i)} & x_4^{(i)} \end{matrix}\right]$$

输出层为

$$o^{(i)} = \left[\begin{matrix} o_1^{(i)} & o_2^{(i)} & o_3^{(i)} \end{matrix}\right]$$

预测概率分布为

$$\hat{y}^{(i)} = \left[\begin{matrix} \hat{y}_1^{(i)} & \hat{y}_2^{(i)} & \hat{y}_3^{(i)} \end{matrix}\right]$$

softmax对回归样本$i$分类的矢量计算表达式为

$$0^{(i)} = x^{(i)}W + b$$

$$\hat{y}^{(i)} = sfotmax(o^{(i)})$$

### 小批量样本分类的矢量计算表达式

给定一个小批量样本，其批量大小为$n$，输入个数（特征数）为$d$，输出个数（类别数）为$q$。设批量特征为$X \in R^{n*d}$。假设softmax回归的权重和偏置参数分别为$W \in R^{d*q}, b \in R^{1*q}$。softmax回归的矢量表达式为

$$O = XW + b$$

$$\hat{Y} = softmax(O)$$

其中加法运算使用了广播机制，$O,\hat{Y} \in R^{n*q}$且两个矩阵的第$i$行分别为样本$i$的输出$o^{i}$和概率分布$\hat{y}^{(i)}$

### 交叉熵损失函数

“交叉熵”（cross-entropy），产生与信息论，简单来说，交叉熵是衡量两个概率分布p和q之间的相似性，其定义如下：

$$ H_{y'}(y) = -\sum_i{y'_i\log y_i} $$

y是预测的概率分布，$y'$是实际的分布（one-hot）

或是

$$H(y^{(i)}, \hat{y}^{(i)}) = -\sum_{j=1}^q y_j^{(i)}\log \hat{y}_j^{(i)}$$

其中带下标的$y_j^{(i)}$是向量$y^{(i)}$中非0即1的元素，需要将它与样本$i$类别的离散数值，即不带下标的$y^{(i)}$区分。向量$y^{(i)}$中只有第$y^{(i)}$个元素$y^{(i)}_{y^{(i)}}$为1，其余全是0。也就是说，交叉熵只关心对正确类别的预测概率，因为只要其值足够大，就可以确保分类结果正确。当然，如果一个样本有多个标签时，就不能做这一步简化了。但即便对这种情况，交叉熵同样只关心对图像中出现的物体类别的预测。

交叉熵适合衡量两个概率分布的差异。

### 模型预测及评价

在训练好softmax回归模型后，给定任意一组样本特征，可以预测出每个输出类别的概率。通常，把预测概率最大的类别作为输出类别。如果它与真实的类别一致，说明这次预测是正确的。可以使用准确率（accuracy）来评价模型的表现。它等于正确预测数量与总预测数量之比。
